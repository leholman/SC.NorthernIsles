# First we combine files from each lane
#we are using the following expression to combine gz files, it is slower than simply glueing them together but I think the compression is better 
#zcat file1.gz file2.gz file3.gz | gzip -c > allfiles-zcat.gz


cd rawdata

find . -type f -name "*R1_001.fastq.gz" -printf '%P\n' | awk -F '_' '{print $1"_"$2}' | sort | uniq |

while IFS= read -r i; do
sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --time=03:00:00
#SBATCH --mem-per-cpu=60G
cd /home/gwm297/data/SeaChange_WP2/0.rawdata
echo "Merging R1 for $i"
echo "Current directory: $(pwd)"
echo "Files for R1: ${i}_L001_R1_001.fastq.gz ${i}_L002_R1_001.fastq.gz ${i}_L003_R1_001.fastq.gz ${i}_L004_R1_001.fastq.gz"
zcat ${i}_L001_R1_001.fastq.gz ${i}_L002_R1_001.fastq.gz ${i}_L003_R1_001.fastq.gz ${i}_L004_R1_001.fastq.gz | gzip -c > ${i}_R1.fastq.gz
EOL

sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=03:00:00
#SBATCH --mem-per-cpu=60G
cd /home/gwm297/data/SeaChange_WP2/0.rawdata
echo "Merging R2 for $i"
echo "Current directory: $(pwd)"
echo "Files for R2: ${i}_L001_R2_001.fastq.gz ${i}_L002_R2_001.fastq.gz ${i}_L003_R2_001.fastq.gz ${i}_L004_R2_001.fastq.gz"
zcat ${i}_L001_R2_001.fastq.gz ${i}_L002_R2_001.fastq.gz ${i}_L003_R2_001.fastq.gz ${i}_L004_R2_001.fastq.gz | gzip -c > ${i}_R2.fastq.gz
EOL
done

mv *R1.fastq.gz ../1.concatdata
mv *R2.fastq.gz ../1.concatdata
cd ../1.concatdata


# Trimming and QC of raw demultiplexed reads 
## VSEARCH v2.22.1_linux_x86_64
## fastp 0.23.4

find . -type f -name "*R1.fastq.gz" -printf '%P\n' | awk -F '_' '{print $1"_"$2}' | sort | uniq |
while IFS= read -r i; do

sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=03:00:00
#SBATCH --mem=100G
cd /home/gwm297/data/SeaChange_WP2/1.concatdata
module load fastp
module load vsearch
echo "Trimming $i"
fastp  -i ${i}_R1.fastq.gz -I ${i}_R2.fastq.gz -m --merged_out ${i}.trim.fastq -V --detect_adapter_for_pe -D --dup_calc_accuracy 5  -g -x -q 30 -e 25 -l 25 -y -c -p -h ${i}.fastp.report.html -w 8
echo "Dedup $i"
vsearch --fastx_uniques ${i}.trim.fastq --fastqout ${i}.trim.vs.fastq --minseqlength 25 --strand both
gzip ${i}.trim.vs.fastq
EOL
done;


mv *.trim.vs.fastq.gz ../2.filtereddata/


## Lets summarise the results 

find . -type f -name "*.trim.vs.fastq.gz" -printf '%P\n' | awk -F '_' '{print $1"_"$2}' | sort | uniq |
while IFS= read -r i; do

sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --time=0:10:00
#SBATCH --mem=10G

cd /home/gwm297/data/SeaChange_WP2/1.concatdata
read_count=\$(zcat "${i}" | grep -c '^@')
echo "${i},\$read_count" >> vsearch.counts.csv
EOL

done


for fastq_gz_file in *.fastq.gz; do
    # Get the file name without the path
    file_name=$(basename "$fastq_gz_file")

    # Use zcat to decompress the file, grep to count lines starting with "@"
    read_count=$(zcat "$fastq_gz_file" | grep -c '^@')

    # Print the result
    echo "File: $file_name, Read Count: $read_count"
done

sort -t',' -k1,1 -o vsearch.counts.csv vsearch.counts.csv 

echo "Sample,sequencing,duplication rate,raw reads,fastP filtered reads,too short reads,low complexity,low quality,GC content,Insert Size Peak,Sample2,dedupReads"> data.summary.output.csv

paste -d ',' \
  <(ls *html | cut -f1 -d"." ) \
  <(grep 'sequencing' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 1') \
  <(grep 'duplication rate' *html | cut -f5 -d">" | cut -f1 -d"%") \
  <(grep 'total reads' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 1') \
  <(grep 'total reads' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 0') \
  <(grep 'reads too short' *html | cut -f5 -d">" | cut -f1 -d"<" | cut -f2 -d"(" | cut -f1 -d"%" ) \
  <(grep 'low complexity' *html | cut -f5 -d">" | cut -f1 -d"<" | cut -f2 -d"(" | cut -f1 -d"%") \
  <(grep 'low quality' *html | cut -f5 -d">" | cut -f2 -d"(" | cut -f1 -d"%" ) \
  <(grep 'GC content' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 0') \
  <(grep 'Insert size peak' *html | cut -f5 -d">" | cut -f1 -d"<" ) \
  <(cut -d',' -f1 vsearch.counts.csv) \
  <(cut -d',' -f2 vsearch.counts.csv) \
  >> data.summary.output.csv



##### we are here 

#Make a list of databases
find -L /datasets/mjolnir_databases/holi_db -type f -name '*.bt2l' -exec readlink -f {} \; | sed -E 's/\.rev\.[0-9]+\.?bt2l$|\.?[0-9]+\.?bt2l$//' | sort -u > databases.txt
find -L /datasets/mjolnir_databases/holi_db -type f -name '*.bt2' -exec readlink -f {} \; | sed -E 's/\.rev\.[0-9]+\.?bt2$|\.?[0-9]+\.?bt2$//' | sort -u >> databases.txt


find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sort > samples.txt



# Define the input files
sample_list="samples.txt"
database_list="databases2.txt"

# Loop through sample names
while IFS= read -r file; do
    # Loop through database names
    while IFS= read -r DB; do
        # Generate the command
        command="bowtie2 -k 1000 -t -x $DB -U $file --no-unal --threads 24  | samtools view -bS - > $(basename $file).$(basename $DB).bam"
        # Print the command to a text file
        echo "$command" >> commands2.txt
    done < databases2.txt
done < samples.txt



##### slurm command bash script (sbatch script) OBS change array 1-46 should be the number of lines in the command.txt file

#!/bin/bash
#SBATCH --job-name NorthMap
#SBATCH --nodes=1
#SBATCH --cpus-per-task=25
#SBATCH --time=03:00:00
#SBATCH --mem=300G
#SBATCH --array=1-2250
#SBATCH --export=ALL
#SBATCH --output=NorthMap_%A_%a.out

module load bowtie2
module load samtools
echo "SLURM_JOBID: " $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID
echo "SLURM_ARRAY_JOB_ID: " $SLURM_ARRAY_JOB_ID
cd /projects/mjolnir1/people/gwm297/SeaChange_WP2/3.mapping/
cmd=$(awk -v line=$SLURM_ARRAY_TASK_ID 'NR == line {print $0}' commands.txt)

# Execute the command or script
eval "$cmd"

### execute the above sbatch file



### Now we merge all the files with the below 

cat samples.txt |
while IFS= read -r i; do

sbatch <<EOL
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --cpus-per-task=25
#SBATCH --time=3-0:00:00
#SBATCH --mem=600G
#SBATCH --job-name Merge

module load samtools
cd /projects/mjolnir1/people/gwm297/SeaChange_WP2/3.mapping
filename=\$(basename "${i}")
filename2=\$(echo "${i}" | sed -n -E 's/.*-(WP2-[0-9]+|ExrNTC|LibPTC|ExrPTC|LibNTC)(_S[0-9]+)?\.trim\.vs\.fastq\.gz/\1\2/p')
echo \$filename
echo \$filename2
samtools merge -f ../4.merged/\$filename2.merged.sam.gz \$filename*.bam -@ 24 && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step1
echo 'step 1 complete'

cd ../4.merged

samtools view --threads 24 -H \$filename2.merged.sam.gz | gzip > \$filename2.merged.Header.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step2
echo 'step 2 complete'

samtools view --threads 24 \$filename2.merged.sam.gz | gzip > \$filename2.merged.alignment.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step3
echo 'step 3 complete'

/projects/mjolnir1/people/gwm297/metagenomicsTools/gzsort/gz-sort -S 100G -P 10 \$filename2.merged.alignment.sam.gz \$filename2.merged.alignment.sort.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step4
echo 'step 4 complete'

zcat \$filename2.merged.Header.sam.gz \$filename2.merged.alignment.sort.sam.gz | samtools view -h -o \$filename2.merged.sort.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step5
echo 'FINISHED'

rm \$filename2.merged.Header.sam.gz \$filename2.merged.alignment.sam.gz \$filename2.merged.alignment.sort.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step6delete

EOL

done

###### EVERYHTING IS DONE NOW

##metaDMG activate!  


find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sed -E 's/.*-(WP[0-9]+|ExrNTC|LibPTC|ExrPTC)231009[0-9]+-(WP[0-9]+|ExrNTC|LibPTC|ExrPTC)_S([0-9]+)\.trim\.vs\.fastq\.gz/\1\2-\3/'
find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sed -n -E '/WP2/s/.*\/([0-9]+-WP2-[0-9]+)_S[0-9]+.trim.vs.fastq.gz$/WP2-\1/p; /WP2/!s/.*\/([^-]+)-[0-9]+_S[0-9]+.trim.vs.fastq.gz$/\1/p'
find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sed -n 's/.*-\(WP2-[0-9]\+\|ExrNTC\|LibPTC\|ExrPTC_S[0-9]\+\)\+\.trim\.vs\.fastq\.gz/\1/p'


/projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/LV7009024928-LV7005367174-WP2-008_S8.trim.vs.fastq.gz

ls /projects/mjolnir1/people/gwm297/SeaChange_WP2/3.mapping/LV7009024914-LV7005367200-WP2-023_S23.trim.vs.fastq.gz.*.bam

## Merge all 

time cat sample.list | parallel -j 2 'samtools merge  {}.merged.sam.gz {}*.bam -@ 24'
time cat sample.list | parallel -j 2 'samtools view --threads 24  -H {}.merged.sam.gz | gzip > {}.merged.Header.sam.gz'
time cat sample.list | parallel -j 2 'samtools view --threads 24 {}.merged.sam.gz | gzip > {}.merged.alignment.sam.gz'
time cat sample.list | parallel -j 2 '/projects/lundbeck/people/npl206/programmes/gz-sort/gz-sort -S 30G -P 10 {}.merged.alignment.sam.gz {}.merged.alignment.sort.sam.gz'
time cat sample.list | parallel -j 2 'zcat {}.merged.Header.sam.gz {}.merged.alignment.sort.sam.gz | samtools view -h -o {}.merged.sort.sam.gz'
#rm *.merged.Header.sam.gz *.merged.alignment.sam.gz *.merged.alignment.sort.sam.gz (edited) 






