###############################################################################
# Bioinformatics script
# SeaChange WP2 Northern Isles Ancient Metagenomics
# Luke E Holman
# July 2025
###############################################################################

#### 1.0 Quality check and preprocessing

## First we run this script to QC, merge and discard poor sequences before mapping
ls *_R1.fastq.gz | sed 's/_R1.fastq.gz$//' | sort | uniq |
while IFS= read -r i; do

sbatch <<EOL
#!/bin/bash
#SBATCH --job-name=Prep.1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=03:00:00
#SBATCH --mem=100G
cd /home/gwm297/data/SeaChange_WP2/0.rawdata/

module purge
module load conda
source ~/.bashrc
conda activate sga-env
module load fastp
module load vsearch
echo "Trimming $i"
fastp  -i ${i}_R1.fastq.gz -I ${i}_R2.fastq.gz -m --merged_out ${i}.trim.fastq -V --detect_adapter_for_pe -D --dup_calc_accuracy 5  -g -x -q 30 -e 25 -l 30 -y -c -p -h ${i}.fastp.report.html -w 8
echo "Dedup $i"
vsearch --fastx_uniques ${i}.trim.fastq --fastqout ${i}.trim.vs.fastq --minseqlength 30 --strand both
sga preprocess --dust-threshold=4 -m 30 ${i}.trim.vs.fastq  -o ${i}.trim.vs.d1.fastq
gzip ${i}.trim.vs.d1.fastq
read_count=\$(zcat "${i}.trim.vs.d1.fastq.gz" | grep -c '^@')
echo "${i},\$read_count" >> vsearch.counts.csv
mv ${i}.trim.vs.d1.fastq.gz ../1.premapping/${i}.trim.vs.d1.fastq.gz
EOL
done;

### now we extract all the outputs from the html files
ls *.fastp.report.html | sed 's/.fastp.report.html//' | sort > sample_names.txt
sort -t',' -k1,1 vsearch.counts.csv > sorted.vsearch.counts.csv

echo "Sample,sequencing,duplication rate,raw reads,fastP filtered reads,too short reads,low complexity,low quality,GC content,Insert Size Peak,no overlap percent,ID2,premapping reads" > data.summary.output.csv

paste -d ',' \
  sample_names.txt \
  <(for f in $(cat sample_names.txt); do val=$(grep 'sequencing' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR==1'); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'duplication rate' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"%"); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'total reads' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR==1'); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'total reads' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR==2'); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'reads too short' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"<" | cut -f2 -d"(" | cut -f1 -d"%"); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'low complexity' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"<" | cut -f2 -d"(" | cut -f1 -d"%"); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'low quality' $f.fastp.report.html | cut -f5 -d">" | cut -f2 -d"(" | cut -f1 -d"%"); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'GC content' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR==2'); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'Insert size peak' $f.fastp.report.html | cut -f5 -d">" | cut -f1 -d"<"); echo "${val:-NA}"; done) \
  <(for f in $(cat sample_names.txt); do val=$(grep 'This estimation is based on paired-end overlap analysis' $f.fastp.report.html | sed -E 's/.* ([0-9]+\.[0-9]+)% .*/\1/'); echo "${val:-NA}"; done) \
  <(cut -d',' -f1 sorted.vsearch.counts.csv) \
  <(cut -d',' -f2 sorted.vsearch.counts.csv) \
  >> data.summary.output.csv

#### 2.1 Eukaryotic Mapping 
cd ../2.mapping
find -L /datasets/globe_databases/holi_db \( -name '*.bt2' -o -name '*.bt2l' \) |

# ── 1. Pick newest date for each family (refseq/plant, nt, etc.) ──
awk -F/ '
{
  fam  = $(NF-3) "/" $(NF-2);   # e.g. refseq/plant
  date = $(NF-1);               # e.g. 20250505
  line = $0;                    # full path including part-number & extension

  # remember newest date
  if (date > newest[fam]) newest[fam] = date;

  # keep every line, keyed by fam|date
  bucket[fam, date] = bucket[fam, date] ? bucket[fam, date] ORS line : line;
}
END {
  # spit out only lines that belong to the newest date for each family
  for (key in bucket) {
    split(key, a, SUBSEP); fam = a[1]; date = a[2];
    if (date == newest[fam]) print bucket[key];
  }
}' |

# ── 2. Now strip .rev.N.bt2[l] or .N.bt2[l] so we have a clean prefix ──
sed -E 's/\.rev\.[0-9]+\.?bt2l?$|\.?[0-9]+\.?bt2l?$//' |

# ── 3. Remove any duplicates that are left ──
sort -u > databases.txt

## find samples
find /projects/seachange/people/gwm297/SeaChange_WP2/1.premapping/ -type f -name '*fastq.gz' | sort > samples.txt


# Define the input files
sample_list="samples.txt"
database_list="databases.txt"

# Empty the output file
> commands.txt

# Loop through sample names
while IFS= read -r file; do
    # Extract base sample name without suffix
    loopfile=$(basename "$file" | sed -E 's/.trim.vs.d1.fastq.gz$//')
 mkdir $loopfile
    # Loop through database names
    while IFS= read -r DB; do
        # Generate the command
        command="bowtie2 -k 1000 -t -x $DB -U $file --no-unal --threads 24 | samtools view -bS - > ${loopfile}/${loopfile}.$(basename $DB).bam"
        
        # Print the command to a text file
        echo "$command" >> commands.txt
    done < "$database_list"
done < "$sample_list"

## test boi
#!/bin/bash
#SBATCH --job-name NorthMap
#SBATCH --nodes=1
#SBATCH --cpus-per-task=25
#SBATCH --time=03:00:00
#SBATCH --mem=300G
#SBATCH --array=23,145,982,1245,2003,3567,4001,5032,6000,6999
#SBATCH --export=ALL
#SBATCH --output=NorthMap_%A_%a.out

module load bowtie2
module load samtools
echo "SLURM_JOBID: " $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID
echo "SLURM_ARRAY_JOB_ID: " $SLURM_ARRAY_JOB_ID
cd /projects/seachange/people/gwm297/SeaChange_WP2/2.mapping/
cmd=$(awk -v line=$SLURM_ARRAY_TASK_ID 'NR == line {print $0}' commands.txt)

# Execute the command or script
eval "$cmd"

######## we are here 22 July - set off mapping with not NT databases 
grep -v "/nt/" databases.txt > databases.noNT.txt

# Define the input files
sample_list="samples.txt"
database_list="databases.noNT.txt"

# Empty the output file
> commands.txt

# Loop through sample names
while IFS= read -r file; do
    # Extract base sample name without suffix
    loopfile=$(basename "$file" | sed -E 's/.trim.vs.d1.fastq.gz$//')
 mkdir $loopfile
    # Loop through database names
    while IFS= read -r DB; do
        # Generate the command
        command="bowtie2 -k 1000 -t -x $DB -U $file --no-unal --threads 24 | samtools view -bS - > ${loopfile}/${loopfile}.$(basename $DB).bam"
        
        # Print the command to a text file
        echo "$command" >> commands.txt
    done < "$database_list"
done < "$sample_list"


##### slurm command bash script (sbatch script) OBS change array 1-46 should be the number of lines in the command.txt file

#!/bin/bash
#SBATCH --job-name NorthMap25
#SBATCH --nodes=1
#SBATCH --cpus-per-task=25
#SBATCH --time=04:00:00
#SBATCH --mem=300G
#SBATCH --array=1-6048
#SBATCH --export=ALL
#SBATCH --output=NorthMap_%A_%a.out

module load bowtie2
module load samtools
echo "SLURM_JOBID: " $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID
echo "SLURM_ARRAY_JOB_ID: " $SLURM_ARRAY_JOB_ID
cd /projects/seachange/people/gwm297/SeaChange_WP2/2.mapping/
cmd=$(awk -v line=$SLURM_ARRAY_TASK_ID 'NR == line {print $0}' commands.txt)

# Execute the command or script
eval "$cmd"


### basement

# First we combine files from each lane
#we are using the following expression to combine gz files, it is slower than simply glueing them together but I think the compression is better 
#zcat file1.gz file2.gz file3.gz | gzip -c > allfiles-zcat.gz


cd rawdata

find . -type f -name "*R1_001.fastq.gz" -printf '%P\n' | awk -F '_' '{print $1"_"$2}' | sort | uniq |

while IFS= read -r i; do
sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --time=03:00:00
#SBATCH --mem-per-cpu=60G
cd /home/gwm297/data/SeaChange_WP2/0.rawdata
echo "Merging R1 for $i"
echo "Current directory: $(pwd)"
echo "Files for R1: ${i}_L001_R1_001.fastq.gz ${i}_L002_R1_001.fastq.gz ${i}_L003_R1_001.fastq.gz ${i}_L004_R1_001.fastq.gz"
zcat ${i}_L001_R1_001.fastq.gz ${i}_L002_R1_001.fastq.gz ${i}_L003_R1_001.fastq.gz ${i}_L004_R1_001.fastq.gz | gzip -c > ${i}_R1.fastq.gz
EOL

sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=03:00:00
#SBATCH --mem-per-cpu=60G
cd /home/gwm297/data/SeaChange_WP2/0.rawdata
echo "Merging R2 for $i"
echo "Current directory: $(pwd)"
echo "Files for R2: ${i}_L001_R2_001.fastq.gz ${i}_L002_R2_001.fastq.gz ${i}_L003_R2_001.fastq.gz ${i}_L004_R2_001.fastq.gz"
zcat ${i}_L001_R2_001.fastq.gz ${i}_L002_R2_001.fastq.gz ${i}_L003_R2_001.fastq.gz ${i}_L004_R2_001.fastq.gz | gzip -c > ${i}_R2.fastq.gz
EOL
done

mv *R1.fastq.gz ../1.concatdata
mv *R2.fastq.gz ../1.concatdata
cd ../1.concatdata


# Trimming and QC of raw demultiplexed reads 
## VSEARCH v2.22.1_linux_x86_64
## fastp 0.23.4

find . -type f -name "*R1.fastq.gz" -printf '%P\n' | awk -F '_' '{print $1"_"$2}' | sort | uniq |
while IFS= read -r i; do

sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=03:00:00
#SBATCH --mem=100G
cd /home/gwm297/data/SeaChange_WP2/1.concatdata
module load fastp
module load vsearch
echo "Trimming $i"
fastp  -i ${i}_R1.fastq.gz -I ${i}_R2.fastq.gz -m --merged_out ${i}.trim.fastq -V --detect_adapter_for_pe -D --dup_calc_accuracy 5  -g -x -q 30 -e 25 -l 25 -y -c -p -h ${i}.fastp.report.html -w 8
echo "Dedup $i"
vsearch --fastx_uniques ${i}.trim.fastq --fastqout ${i}.trim.vs.fastq --minseqlength 25 --strand both
gzip ${i}.trim.vs.fastq
EOL
done;


mv *.trim.vs.fastq.gz ../2.filtereddata/


## Lets summarise the results 

find . -type f -name "*.trim.vs.fastq.gz" -printf '%P\n' | awk -F '_' '{print $1"_"$2}' | sort | uniq |
while IFS= read -r i; do

sbatch <<EOL
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --time=0:10:00
#SBATCH --mem=10G

cd /home/gwm297/data/SeaChange_WP2/1.concatdata
read_count=\$(zcat "${i}" | grep -c '^@')
echo "${i},\$read_count" >> vsearch.counts.csv
EOL

done


for fastq_gz_file in *.fastq.gz; do
    # Get the file name without the path
    file_name=$(basename "$fastq_gz_file")

    # Use zcat to decompress the file, grep to count lines starting with "@"
    read_count=$(zcat "$fastq_gz_file" | grep -c '^@')

    # Print the result
    echo "File: $file_name, Read Count: $read_count"
done

sort -t',' -k1,1 -o vsearch.counts.csv vsearch.counts.csv 

echo "Sample,sequencing,duplication rate,raw reads,fastP filtered reads,too short reads,low complexity,low quality,GC content,Insert Size Peak,Sample2,dedupReads"> data.summary.output.csv

paste -d ',' \
  <(ls *html | cut -f1 -d"." ) \
  <(grep 'sequencing' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 1') \
  <(grep 'duplication rate' *html | cut -f5 -d">" | cut -f1 -d"%") \
  <(grep 'total reads' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 1') \
  <(grep 'total reads' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 0') \
  <(grep 'reads too short' *html | cut -f5 -d">" | cut -f1 -d"<" | cut -f2 -d"(" | cut -f1 -d"%" ) \
  <(grep 'low complexity' *html | cut -f5 -d">" | cut -f1 -d"<" | cut -f2 -d"(" | cut -f1 -d"%") \
  <(grep 'low quality' *html | cut -f5 -d">" | cut -f2 -d"(" | cut -f1 -d"%" ) \
  <(grep 'GC content' *html | cut -f5 -d">" | cut -f1 -d"<" | awk 'NR % 2 == 0') \
  <(grep 'Insert size peak' *html | cut -f5 -d">" | cut -f1 -d"<" ) \
  <(cut -d',' -f1 vsearch.counts.csv) \
  <(cut -d',' -f2 vsearch.counts.csv) \
  >> data.summary.output.csv



##### we are here 

#Make a list of databases
find -L /datasets/globe_databases/holi_db -type f -name '*.bt2l' -exec readlink -f {} \; | sed -E 's/\.rev\.[0-9]+\.?bt2l$|\.?[0-9]+\.?bt2l$//' | sed -E '/\/2023[0-9]{4}\//d' | sort -u > databases.txt
find -L /datasets/globe_databases/holi_db -type f -name '*.bt2' -exec readlink -f {} \; | sed -E 's/\.rev\.[0-9]+\.?bt2$|\.?[0-9]+\.?bt2$//' | sed -E '/\/2023[0-9]{4}\//d' |sort -u >> databases.txt


find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sort > samples.txt



# Define the input files
sample_list="samples.txt"
database_list="databases2.txt"

# Loop through sample names
while IFS= read -r file; do
    # Loop through database names
    while IFS= read -r DB; do
        # Generate the command
        command="bowtie2 -k 1000 -t -x $DB -U $file --no-unal --threads 24  | samtools view -bS - > $(basename $file).$(basename $DB).bam"
        # Print the command to a text file
        echo "$command" >> commands2.txt
    done < databases2.txt
done < samples.txt



##### slurm command bash script (sbatch script) OBS change array 1-46 should be the number of lines in the command.txt file

#!/bin/bash
#SBATCH --job-name NorthMap
#SBATCH --nodes=1
#SBATCH --cpus-per-task=25
#SBATCH --time=03:00:00
#SBATCH --mem=300G
#SBATCH --array=1-2250
#SBATCH --export=ALL
#SBATCH --output=NorthMap_%A_%a.out

module load bowtie2
module load samtools
echo "SLURM_JOBID: " $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID
echo "SLURM_ARRAY_JOB_ID: " $SLURM_ARRAY_JOB_ID
cd /projects/mjolnir1/people/gwm297/SeaChange_WP2/3.mapping/
cmd=$(awk -v line=$SLURM_ARRAY_TASK_ID 'NR == line {print $0}' commands.txt)

# Execute the command or script
eval "$cmd"

### execute the above sbatch file



### Now we merge all the files with the below 

cat samples.txt |
while IFS= read -r i; do

sbatch <<EOL
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --cpus-per-task=25
#SBATCH --time=3-0:00:00
#SBATCH --mem=600G
#SBATCH --job-name Merge

module load samtools
cd /projects/mjolnir1/people/gwm297/SeaChange_WP2/3.mapping
filename=\$(basename "${i}")
filename2=\$(echo "${i}" | sed -n -E 's/.*-(WP2-[0-9]+|ExrNTC|LibPTC|ExrPTC|LibNTC)(_S[0-9]+)?\.trim\.vs\.fastq\.gz/\1\2/p')
echo \$filename
echo \$filename2
samtools merge -f ../4.merged/\$filename2.merged.sam.gz \$filename*.bam -@ 24 && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step1
echo 'step 1 complete'

cd ../4.merged

samtools view --threads 24 -H \$filename2.merged.sam.gz | gzip > \$filename2.merged.Header.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step2
echo 'step 2 complete'

samtools view --threads 24 \$filename2.merged.sam.gz | gzip > \$filename2.merged.alignment.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step3
echo 'step 3 complete'

/projects/mjolnir1/people/gwm297/metagenomicsTools/gzsort/gz-sort -S 100G -P 10 \$filename2.merged.alignment.sam.gz \$filename2.merged.alignment.sort.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step4
echo 'step 4 complete'

zcat \$filename2.merged.Header.sam.gz \$filename2.merged.alignment.sort.sam.gz | samtools view -h -o \$filename2.merged.sort.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step5
echo 'FINISHED'

rm \$filename2.merged.Header.sam.gz \$filename2.merged.alignment.sam.gz \$filename2.merged.alignment.sort.sam.gz && touch /projects/mjolnir1/people/gwm297/SeaChange_WP2/9.monitoring/.\$filename2.Step6delete

EOL

done

### now metaDMG

#!/bin/bash
#SBATCH --job-name mDMG02
#SBATCH --nodes=1
#SBATCH --cpus-per-task=25
#SBATCH --time=6-00:00:00
#SBATCH --mem=300G
#SBATCH --export=ALL
#SBATCH --output=metaDMG05mar.out
module load anaconda3
source activate metaDMG
export LD_LIBRARY_PATH="/home/gwm297/.conda/envs/metaDMG/lib:$LD_LIBRARY_PATH"
cd /home/gwm297/data/SeaChange_WP2/5.metaDMG
metaDMG compute config2.yaml


#!/bin/bash
#SBATCH --job-name mDMG.H
#SBATCH --nodes=1
#SBATCH --cpus-per-task=19
#SBATCH --time=6-00:00:00
#SBATCH --mem=400G
#SBATCH --export=ALL
#SBATCH --output=metaDMG05mar2.out
module load anaconda3
source activate metaDMG
export LD_LIBRARY_PATH="/home/gwm297/.conda/envs/metaDMG/lib:$LD_LIBRARY_PATH"
cd /home/gwm297/data/SeaChange_WP2/5.metaDMG
metaDMG compute configH.yaml





find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sed -E 's/.*-(WP[0-9]+|ExrNTC|LibPTC|ExrPTC)231009[0-9]+-(WP[0-9]+|ExrNTC|LibPTC|ExrPTC)_S([0-9]+)\.trim\.vs\.fastq\.gz/\1\2-\3/'
find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sed -n -E '/WP2/s/.*\/([0-9]+-WP2-[0-9]+)_S[0-9]+.trim.vs.fastq.gz$/WP2-\1/p; /WP2/!s/.*\/([^-]+)-[0-9]+_S[0-9]+.trim.vs.fastq.gz$/\1/p'
find /projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/ -type f -name '*.trim.vs.fastq.gz' | sed -n 's/.*-\(WP2-[0-9]\+\|ExrNTC\|LibPTC\|ExrPTC_S[0-9]\+\)\+\.trim\.vs\.fastq\.gz/\1/p'


/projects/mjolnir1/people/gwm297/SeaChange_WP2/2.filtereddata/LV7009024928-LV7005367174-WP2-008_S8.trim.vs.fastq.gz

ls /projects/mjolnir1/people/gwm297/SeaChange_WP2/3.mapping/LV7009024914-LV7005367200-WP2-023_S23.trim.vs.fastq.gz.*.bam

## Merge all 

time cat sample.list | parallel -j 2 'samtools merge  {}.merged.sam.gz {}*.bam -@ 24'
time cat sample.list | parallel -j 2 'samtools view --threads 24  -H {}.merged.sam.gz | gzip > {}.merged.Header.sam.gz'
time cat sample.list | parallel -j 2 'samtools view --threads 24 {}.merged.sam.gz | gzip > {}.merged.alignment.sam.gz'
time cat sample.list | parallel -j 2 '/projects/lundbeck/people/npl206/programmes/gz-sort/gz-sort -S 30G -P 10 {}.merged.alignment.sam.gz {}.merged.alignment.sort.sam.gz'
time cat sample.list | parallel -j 2 'zcat {}.merged.Header.sam.gz {}.merged.alignment.sort.sam.gz | samtools view -h -o {}.merged.sort.sam.gz'
#rm *.merged.Header.sam.gz *.merged.alignment.sam.gz *.merged.alignment.sort.sam.gz (edited) 






